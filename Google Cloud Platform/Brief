GCP Services: compute, storage, big data and machine learning.

Cloud computing 5 traits-
on-demand self-service (no human intervention needed to get resources)
broad network access 
resource pooling (provider shares resources to customers)
rapid elasticity (get more resources quickly as needed)
measured service (pay only for what u consumed)

colocation brought us cloud computing [physical/colo -> virtualization -> serverless (automatic)]
cloud computing service let you scale your resource use up and down.
false: To get resources from a cloud computing provider, is working with a person at the provider required.
virtualised data centers: IaaS [infrastructure as a service] and PaaS [platform as a service]
IaaS provide raw compute storage and networks. in this model, you pay for what you allocate.
PaaS bind application to libraries that give access to the infrastructure that application needs. you pay for what you use.
SaaS [software as a service] - like googlesearch, gmail, docs and drive - they are consumed directly over the internet by the end users.

When an Internet user sends traffic to a Google resource, Google responds to the user's request from an edge network location that will provide the lowest latency. Google's Edge-caching network cites content close to end users to minimize latency. 
A zone is a deployment area for Google Cloud Platform Resources. For example, when you launch a virtual machine in GCP using Compute Engine, it runs in a zone you specify. Although people think of a zone as being like a GCP Data Center, that's not strictly accurate because a zone doesn't always correspond to a single physical building. Zones are grouped into regions, independent geographic areas, and you can choose what regions your GCP resources are in. All the zones within a region have fast network connectivity among them. Locations within regions usually have round trip network latencies of under five milliseconds. Think of a zone as a single failure domain within a region. As part of building a fault tolerant application, you can spread their resources across multiple zones in a region. That helps protect against unexpected failures. You can run resources in different regions too. Lots of GCP customers do that, both to bring their applications closer to users around the world, and also to protect against the loss of an entire region, say, due to a natural disaster. A few Google Cloud Platform Services support placing resources in what we call a Multi-Region.
Google was the first major Cloud provider to deliver per second billing for its Infrastructure as a Service Compute offering, Google Compute Engine. Fine-grain billing is a big cost savings for workloads that are bursty, which is a lot of them. Many of the best-known GCP services billed by the second, including Compute Engine and Kubernetes Engine. Compute Engine offers automatically applied sustained use discounts which are automatic discounts that you get for running a virtual machine instance for a significant portion of the billing month. Specifically, when you run an instance for more than 25 percent of a month, Compute Engine automatically gives you a discount for every incremental minute you use for that instance. Compute Engines Custom Virtual Machine types lets you fine-tune virtual machines for their applications, which in turn lets you tailor your pricing for your workloads.
GCP services are compatible with open source products. For example, take Cloud Bigtable, a database we'll discuss later. Bigtable uses the interface of the open source database Apache HBase, which gives customers the benefit of code portability. Another example, Cloud Dataproc offers the open source big data environment Hadoop, as a managed service. Google publishes key elements of technology using open source licenses to create ecosystems that provide customers with options other than Google. For example, TensorFlow, an open source software library for machine learning developed inside Google, is at the heart of a strong open source ecosystem. Many GCP technologies provide interoperability. Kubernetes gives customers the ability to mix and match microservices running across different clouds, and Google Stackdriver lets customers monitor workload across multiple cloud providers. 
Both the server boards and the networking equipment in Google data centers are custom designed by Google. Google also designs custom chips, including a hardware security chip called Titan, that's currently being deployed on both servers and peripherals. Google server machines use cryptographic signatures to make sure they are booting the correct software. Google designs and builds its own data centers which incorporate multiple layers of physical security protections. Google's infrastructure provides cryptographic privacy and integrity for remote procedure called data-on-the-network, which is how Google services communicate with each other. The infrastructure automatically encrypts our PC traffic in transit between data centers. Google Central Identity Service, which usually manifests to end users as the Google log-in page, goes beyond asking for a simple username and password. It also intelligently challenges users for additional information based on risk factors such as whether they have logged in from the same device or a similar location in the past. Users can also use second factors when signing in, including devices based on the universal second factor U2F open standard. Most applications at Google access physical storage indirectly via storage services and encryption is built into those services. Google also enables hardware encryption support in hard drives and SSDs. That's how Google achieves encryption at rest of customer data. Google services that want to make themselves available on the Internet register themselves with an infrastructure service called the Google Front End, which checks incoming network connections for correct certificates and best practices.  The GFE also additionally, applies protections against denial of service attacks. The sheer scale of its infrastructure, enables Google to simply absorb many denial of service attacks, even behind the GFEs. Google also has multi-tier, multi-layer denial of service protections that further reduce the risk of any denial of service impact. Inside Google's infrastructure, machine intelligence and rules warn of possible incidents. Google conducts Red Team exercises, simulated attacks to improve the effectiveness of its responses. To guard against phishing attacks against Google employees, employee accounts require use of U2F compatible security keys. To help ensure that code is as secure as possible, Google stores its source code centrally and requires two-party review of new code. Google also gives its developers libraries that keep them from introducing certain classes of security bugs. Externally, Google also runs a vulnerability rewards program, where we pay anyone who is able to discover and inform us of bugs in our infrastructure or applications.
budgets, billing, alerts [BigQuery dataset or Cloud storage bucket], reports, quotas [protect both account owners and the GCP community as a whole. Quotas are designed to prevent the over-consumption of resources, whether because of error or malicious attack. There are two types of quotas: rate quotas and allocation quotas. Both get applied at the level of the GCP project. Rate quotas reset after a specific time. For example, by default, the Kubernetes Engine service sets a quota of a 1000 calls to its API from each GCP project every 100 seconds. After that 100 seconds, the limit is reset. Allocation quotas, on the other hand, govern the number of resources you can have in your projects. For example, by default, each GCP project has a quota allowing it no more than five Virtual Private Cloud networks. Although projects all start with the same quotas, you can change some of them by requesting an increase from Google Cloud support]

fundamental characteristic of devices in a virtualized data center: manageable separately from underlying hardware.
customers who create and run many virtual machines benefits most from billing by the second for cloud resources such as virtual machines.
There are four ways to interact with GCP's management layer: through the web-based console, through the SDK and its command-line tools, through the APIs, and through a mobile app.
Services and APIs are enabled on a per-project basis.
/platform resource hierarchy- All the resources you use, whether they're virtual machines, cloud storage buckets, tables and big query or anything else in GCP are organized into projects. Optionally, these projects may be organized into folders. Folders can contain other folders. All the folders and projects used by your organization can be brought together under an organization node. Projects, folders and organization nodes are all places where the policies can be defined. Some GCP resources let you put policies on individual resources too, like those cloud storage buckets. Resources inherit the policies of their parent resource. The policies implemented at a higher level in this hierarchy can't take away access that's granted at a lower level. For example, suppose that a policy applied on the bookshelf project gives user Pat the right to modify a cloud storage bucket, but a policy at the organization level says that Pat can only view cloud storage buckets not change them.
IAM lets administrators authorize who can take action on specific resources. An IAM policy has a "who" part, a "can do what" part, and an "on which resource" part. The "who" part names the user or users you're talking about. The "who" part of an IAM policy can be defined either by a Google account, a Google group, a Service account, an entire G Suite, or a Cloud Identity domain. The "can do what" part is defined by an IAM role. An IAM role is a collection of permissions. Most of the time, to do any meaningful operations, you need more than one permission. There are three kinds of roles in Cloud IAM- primitive [owner, editor, viewer, billing admin], predefined and customed [A couple cautions about custom roles. First, you have to decide to use custom roles. You'll need to manage their permissions. Some companies decide they'd rather stick with the predefined roles. Second, custom roles can only be used at the project or organization levels. They can't be used at the folder level]. use a service account to give permissions to a Compute Engine virtual machine, rather than to a person. you'd create a service account to authenticate your VM to cloud storage. Service accounts are named with an email address. But instead of passwords, they use cryptographic keys to access resources. a service account is also a resource. So it can have IAM policies on its own attached to it. 
Q) when would u choose organization node. True: when u want to create folders, when u want to apply organization-wide policies centrally; false: when u want to organize resources into project.
/Interacting with GCP
Google Cloud Launcher is a tool for quickly deploying functional software packages on Google Cloud Platform. There's no need to manually configure the software, virtual machine instances, storage or network settings. Although, you can modify many of them before you launch if you like. GCP updates the base images for these software packages to fix critical issues and vulnerabilities. But it doesn't update the software after it's been deployed. Fortunately, you'll have access to the deployed systems, so you can maintain them. 
{launch a solution using cloud marketplace/launcher}/LAMP Stack deployed [checked the site and created a php testpage], used Qwiklabs [cloud console]

False: In Google Cloud IAM: if a policy applied at the project level gives you Owner permissions, your access to an individual resource in that project might be restricted to View permission if someone applies a more restrictive policy directly to that resource. [Policies are a union of those applied on resource itself and those inherited from higher levels in the hierarchy. If a parent policy is less restrictive, it overrides a more restrictive policy applied on the resource. If a parent policy is more restrictive, it does not override a less restrictive policy applied on the resource. Therefore, access granted at a higher level in the hierarchy cannot be taken away by policies applied at a lower level in the hierarchy]
All Google Cloud Platform resources are associated with a project.
Service accounts are used to provide: a way to restrict the actions of a resource (such as a VM) can perform, a way to allow users to act with service account permissions and authentication between GCP services, but not a set of predefined permissions.
primitive roles affect all the resources in a project. predefined roles apply to specific serive in a project.

Compute Engine lets you run virtual machines on Google's global infrastructure. can flexibly reconfigure them and a VM running on Google's cloud has 
/VPC Network
can create a virtual machine instance by using the Google Cloud Platform console or the GCloud command line tool. Speaking of processing power, if you have workloads like machine learning and data processing that can take advantage of GPUs, many GCP zones have GPU's available for you. Just like physical computers need disks, so do VM. You can choose two kinds of persistent storage; standard or SSD. If your application needs high-performance scratch space, you can attach a local SSD, but be sure to store data of permanent value somewhere else because local SSDs content doesn't last past when the VM terminates. That's why the other kinds are called persistent disks. 
Load balancing options: global HTTP(S), global SSL proxy, global TCP proxy, regional and regional internal.
{create a compute engine VM using GCP console and gcloud command line and connect both}/ in Qwiklabs

Google Cloud Load Balancing allows you to balance HTTP-based traffic across multiple Compute Engine regions.
- Google VPC, networks are global and subnets are regional.
An application running in a Compute Engine virtual machine needs high-performance scratch space. Local SSD type storage meets this need.
[application] running a Preemptible VM - a batch job that can be checkpointed and restarted.
A GCP customer wants to load-balance traffic among the back-end VMs that form part of a multi-tier application - regional internal load balancer.
Dedicated interconnect is a Service Level Agreement.

{Cloud Storage}/Google Cloud Platform has other storage options to meet your needs for structured, unstructured, transactional and relational data. core storage options: Cloud Storage, Cloud SQL, Cloud Spanner, Cloud Data Store and Google Big Table.
object storage - unique keys like URLs.
Cloud Storage is comprised of buckets you create and configure and use to hold your storage objects. The storage objects are immutable [You can turn on object versioning on your buckets if you want. If you do, Cloud Storage keeps a history of modifications. That is, it overrides or deletes all of the objects in the bucket. You can list the archived versions of an object, restore an object to an older state or permanently delete a version as needed. If you don't turn on object versioning, new always overrides old], which means that you do not edit them in place but instead you create new versions. Cloud Storage always encrypts your data on the server side before it is written to disk. data in-transit is encrypted using HTTPS. ACLs and automatic delete or archives.
Cloud storage classes: regional, multi-regional, nearline and coldline.
Cloud Storage objects live in buckets. characteristics defined on a per-bucket basis- globally unique name, a geographic location, a default storage class; not- default file type or an encryption-at-rest setting.
False: Cloud Storage is well suited to providing the root file system of a Linux virtual machine. 

/cloud bigtable
False: Each table in NoSQL databases such as Cloud Bigtable has a single schema that is enforced by the database engine itself. 
Some developers think of Cloud Bigtable as a persistent hashtable. that means - each item in the database can be sparsely populated, and is looked up with a single key.

{Cloud SQL and cloud spanner}/Cloud SQL presents a MySQL or PostgreSQL interface to clients. cloud spanner can scale to higher database sizes, uses ANSI SQL 2011 with extensions and offers transactional consistency at global scale.
cloud datastore is another highly scalable NoSQL database like bigtable. One of its main use cases is to store structured data from App Engine apps. You can also build solutions that span App Engine and Compute Engine with Cloud Datastore as the integration point. As you would expect from a fully-managed service, Cloud Datastore automatically handles sharding and replication, providing you with a highly available and durable database that scales automatically to handle load. Unlike Cloud Bigtable, it also offers transactions that affect multiple database rows, and it lets you do SQL-like queries. 
{deploy a web server VM instance, create a cloud storage bucket using gsutil command line, create cloud SQL instance}/in Qwiklabs
cloud storage-
You are developing an application that transcodes large video files. 
often the ingestion point for data being moved into the cloud, and is frequently the long-term storage location for data
cloud bigtable-
You manufacture devices with sensors and need to stream huge amounts of data from these devices to a storage option in the cloud.
cloud datastore-
You are building a small application. If possible, you'd like this application's data storage to be at no additional charge.
cloud spanner-
Your application needs to store data with strong transactional consistency, and you want seamless scaling up. 
true: nearline and coldline assess lower storage feed and additional retrieval fees. False: data in them is not retrievable immediately, they use differently architected-API from other two,  they have lower durability.

You'll likely want to build your applications using lots of Containers, each performing their own function, say using the micro-services pattern. The units of code running in these Containers can communicate with each other over a network fabric. If you build this way, you can make applications modular. They deploy it easily and scale independently across a group of hosts. The host can scale up and down, and start and stop Containers as demand for your application changes, or even as hosts fail and are replaced. A tool that helps you do this well is Kubernetes. Google Cloud offers Cloud Build, a managed service for building Containers. you use a Docker file to specify how your code gets packaged into a Container. 
false: each container has its own instance of an operating system.
Containers are loosely coupled to their environments. that means- containers abstract away unimportant details of their environments, deploying a containerized application consumes less resources and is less error-prone zone than deploying an application in VMs, containers are easy to move around. not - containers don't require any particular runtime binary, containers package the application into equally sized components.
cluster's a set of master components that control the system as a whole and a set of nodes that run containers. In Kubernetes, a node represents a computing instance. In Google Cloud, nodes are virtual machines running in Compute Engine. Whenever Kubernetes deploys a container or a set of related containers, it does so inside an abstraction called a pod. A pod is the smallest deployable unit in Kubernetes. When you choose a rolling update for a deployment and then give it a new version of the software that it manages, Kubernetes will create pods of the new version one-by-one, waiting for each new version pod to become available before destroying one of the old version pods. Rolling updates are a quick way to push out a new version of your application while still sparing your users from experiencing downtime. 
Anthos is a hybrid and multi-cloud solution powered by the latest innovations in distributed systems, and service management software from Google. 
kubernetes cluster is a group of machines where kubernetes can scheddule workloads.
the resources used to build Kubernetes Engine clusters come from compute engine.
Google keeps Kubernetes Engine refreshed with successive versions of Kubernetes.
other features in GKE and kubernetes- configuring health checks, setting session affinity, managing different rollout strategies.
reasons for deploying applications using containers- simpler to migrate workloads, consistency across development, testing, production environments. not- no need to allocate resources in which to run containers, tight coupling between applications and OSs.
Google Cloud Platform provides a secure, high-speed container image storage service for use with Kubernetes Engine.
Kubernetes Engine workloads run in clusters built from compte engine VMs.
{start a kubernetes engine cluster, runa nd deploy a container and scale up}/in qwiklabs

App Engine is a better choice for a web application than for long-running batch processing.
false: App Engine just runs applications; it doesn't offer any services to the applications it runs. 
Google provides App Engine software development kits in several languages, so that you can test your application locally before you upload it to the real App Engine service. The SDKs also provide simple commands for deployment. 
App Engine Flexible Environment applications let their owners control the geographic region where they run.
{deploy a hello world application to App engine}/in qwiklabs
True: It is possible for an App Engine application's daily billing to drop to zero. App Engine manages the hardware and networking infrastructure required to run your code. False: Developers who write for App Engine do not need to code their applications in any particular way to use the service. App Engine requires you to supply or code your own application load balancing and logging services. App Engine charges you based on the resources you pre-allocate rather than based on the resources you use.
advantages of using the App Engine Flexible Environment over App Engine Standard. True: You can install third-party binaries. Your application can write to local disk. You can SSH in to your application. False: Your application can execute code in background threads. Google provides automatic in-place security patches
advantages of using the App Engine Standard Environment over App Engine Flexible. true: Scaling is finer-grained. Billing can drop to zero if your application is idle. Google provides and maintains runtime binaries. False: You can choose any programming language. You can install third-party binaries.
Apigee Edge-
You want to do business analytics and billing on a customer-facing API. 
You want to gradually decompose a pre-existing monolithic application, not implemented in GCP, into microservices. 
cloud endpoints- you want to support developers who are building services in GCP through API logging and monitoring. 
a developer choose to store source code in Cloud Source Repositories to reduce work and keep code private to the GCP project [not to have control over hosting infrastructure]. 

advantage of putting event-driven components of your application into Cloud Functions - Cloud Functions handles scaling these components seamlessly. [Your code executes whenever an event triggers it, no matter whether it happens rarely or many times per second. That means you don't have to provision compute resources to handle these operations]. not- Cloud Functions means that processing always happens free of charge.
/o->quiz
